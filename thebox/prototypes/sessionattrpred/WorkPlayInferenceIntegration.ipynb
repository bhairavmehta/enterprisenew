{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and uncompress\n",
    "# reference: https://stackoverflow.com/questions/15352668/download-and-decompress-gzipped-file-in-memory\n",
    "\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def download_file(file_url: str, out_file_name: str = None):\n",
    "    \"\"\"\n",
    "    Download file from a given URL. If it is GZiped (has .gz), \n",
    "    it will unzip it. Return the downloaded file name.\n",
    "    \"\"\"\n",
    "    if out_file_name is None:\n",
    "        out_file_name = file_url[file_url.rfind(\"/\")+1:]\n",
    "    if out_file_name[-3:] == \".gz\":\n",
    "        out_file_name = out_file_name[:-3]\n",
    "\n",
    "    response = urllib.request.urlopen(file_url)\n",
    "    with open(out_file_name, 'wb') as outfile:\n",
    "        outfile.write(gzip.decompress(response.read()))\n",
    "        \n",
    "    return out_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.onnx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file(\"http://jerrylia-lx-1.guest.corp.microsoft.com:8081/model.onnx.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_file = 'model.onnx'\n",
    "session = onnxruntime.InferenceSession(onnx_model_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input(s):\n",
      "Input #0 name  :app\n",
      "Input #0 shape :['N', 141]\n",
      "Input #0 type  :tensor(float)\n",
      "Input #1 name  :url\n",
      "Input #1 shape :['N', 5]\n",
      "Input #1 type  :tensor(float)\n",
      "Output:\n",
      "Input #0 name  :dense_5/Sigmoid:0\n",
      "Input #0 shape :['N', 1]\n",
      "Input #0 type  :tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# Verify the input/output of the model\n",
    "print(\"Input(s):\")\n",
    "for (i, inp) in enumerate(session.get_inputs()):\n",
    "    print(f\"Input #{i} name  :{inp.name}\")\n",
    "    print(f\"Input #{i} shape :{inp.shape}\")\n",
    "    print(f\"Input #{i} type  :{inp.type}\")\n",
    "\n",
    "print(\"Output:\")\n",
    "for (i, outp) in enumerate(session.get_outputs()):\n",
    "    print(f\"Input #{i} name  :{outp.name}\")\n",
    "    print(f\"Input #{i} shape :{outp.shape}\")\n",
    "    print(f\"Input #{i} type  :{outp.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Processor code\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "class UrlProcessor(object):\n",
    "    \n",
    "    num_train: int = 1000000\n",
    "    dict_file: str = \"URL.csv\"\n",
    "    MAX_NB_WORDS = 1e6\n",
    "    max_token_len: int = 5\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tokenizer():\n",
    "        tokenizer_nltk = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(['http', 'https', 'www', 'com', 'html', 'org', 'ru', 'jp', 'uk', 'ca', '//'])\n",
    "        return tokenizer_nltk, stop_words\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokenizer_nltk, self.stop_words = UrlProcessor.get_tokenizer()\n",
    "        self.tokenzier_keras = self.cache_url_dict()\n",
    "\n",
    "    def tokenize(self, raw_url: str):\n",
    "        url_tokens = self.tokenizer_nltk.tokenize(raw_url)\n",
    "        url_tokens_clean = [word for word in url_tokens if word not in self.stop_words]\n",
    "        return url_tokens_clean\n",
    "\n",
    "    def cache_url_dict(self):\n",
    "        url_df = pd.read_csv(self.dict_file, header=None)\n",
    "        url_df.columns = ['index', 'url', 'category']\n",
    "        url_df.dropna(inplace=True)\n",
    "        url_train_df = url_df.sample(n=self.num_train, random_state=1)\n",
    "        raw_docs_train = url_df['url'].apply(lambda u: self.tokenize(u))\n",
    "        # NOTE: why 2 passes at all?\n",
    "        processed_docs_train = []\n",
    "        processed_docs_test = []\n",
    "        for doc in raw_docs_train:\n",
    "            tokens = self.tokenizer_nltk.tokenize(\" \".join(doc))\n",
    "            filtered = [word for word in tokens if word not in self.stop_words]\n",
    "            processed_docs_train.append(\" \".join(filtered))\n",
    "        tokenizer_keras = Tokenizer(num_words=self.MAX_NB_WORDS, lower=True, char_level=False)\n",
    "        tokenizer_keras.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "        word_index = tokenizer_keras.word_index\n",
    "        print(\"dictionary size: \", len(word_index))\n",
    "        return tokenizer_keras\n",
    "        \n",
    "    def process_url(self, raw_url: str):\n",
    "        url_tokens_clean = self.tokenize(raw_url)\n",
    "        return self.tokenzier_keras.texts_to_sequences([url_tokens_clean])[0][0:self.max_token_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size:  1068970\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "urlProc = UrlProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[498, 543298, 17602, 3123, 277]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlProc.process_url(\"https://microsoft.visualstudio.com/OSGData/_git/enterprise.mhhd?path=%2Fthebox%2Fprototypes%2Fsessionattrpred%2Fmultimodal_dnn_baseline.ipynb&version=GBdev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "class AppProcessor(object):\n",
    "    \n",
    "    num_train: int = 1000000\n",
    "    dict_file: str = \"AppResult.csv\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.encoder = self.cache_app_transform()\n",
    "    \n",
    "    def cache_app_transform(self):\n",
    "        \n",
    "        app_session_df = pd.read_csv(self.dict_file)\n",
    "        app_session_df.dropna(inplace=True)\n",
    "\n",
    "        for i in range (10):\n",
    "            app_session_df = pd.concat([app_session_df,app_session_df], axis=0)\n",
    "\n",
    "        # NOTE: seed?\n",
    "        app_train_df = app_session_df.sample(n=self.num_train, random_state=1)\n",
    "        #print(app_train_df)\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        enc.fit(app_train_df['Process'].values.reshape(-1,1))\n",
    "\n",
    "        print(f\"Encoder created with category count: {enc.categories_[0].shape}\")\n",
    "        return enc\n",
    "\n",
    "    def process_app(self, app_name: str):\n",
    "        return self.encoder.transform(np.array([app_name]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder created with category count: (141,)\n",
      "Wall time: 506 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "appProc = AppProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appProc.process_app(\"chrome.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_app_name = session.get_inputs()[0].name\n",
    "input_url_name = session.get_inputs()[1].name\n",
    "output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_app = \"chrome.exe\"\n",
    "test_url = \"https://microsoft.visualstudio.com/OSGData/_git/enterprise.mhhd?path=%2Fthebox%2Fprototypes%2Fsessionattrpred%2Fmultimodal_dnn_baseline.ipynb&version=GBdev\"\n",
    "test_app_in = np.array(appProc.process_app(test_app)).astype(np.float32).reshape(1, -1)\n",
    "test_url_in = np.array([urlProc.process_url(test_url)]).astype(np.float32).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_app shape: (1, 141)\n",
      "test_url shape: (1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_app shape: {test_app_in.shape}\")\n",
    "print(f\"test_url shape: {test_url_in.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_app_in.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = session.run(\n",
    "    [output_name], \n",
    "    {input_app_name: test_app_in, \n",
    "     input_url_name: test_url_in}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_threshold = 0.4\n",
    "scorer = lambda x: 1 if x > score_threshold else 0\n",
    "labels = { 1: \"work\", 0: \"play\"}\n",
    "labels[scorer(result[0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
