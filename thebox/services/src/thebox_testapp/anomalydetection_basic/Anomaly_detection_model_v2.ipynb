{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this notebook, we train take a sequence as an input, encode it with a deep autoencoder, and \n",
    "output the element-wise encoding error. This output can be used for calculating a metrics for anomaly detection.\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_reading(dim, noise_scale):\n",
    "    w = np.linspace(2.6, 5.8, dim) # frequency\n",
    "    s = np.linspace(1.2, 9.4, dim)  # phase\n",
    "    \n",
    "    t = np.linspace(0, 10.0, 501)\n",
    "    reading = []\n",
    "    for i in range(dim):\n",
    "        signal = np.sin(w[i]*t + s[i])\n",
    "        noise = np.random.normal(0, noise_scale, len(t))\n",
    "        reading.append( signal + noise )\n",
    "    reading = np.array(reading)\n",
    "    return reading\n",
    "\n",
    "\n",
    "def get_normal_data(dim, sequence_length, normal_sample_size, noise_scale, **kwargs):\n",
    "    '''\n",
    "    x contains the sequences draw from the normal readings. x has the shape of (normal_sample_size, sequence_length, dim)\n",
    "    y contains the label of if the sequence is an anomaly. In this case, all labels in y are 0.\n",
    "    '''\n",
    "    reading = get_normal_reading(dim, noise_scale)\n",
    "      \n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(normal_sample_size):\n",
    "        i = np.random.randint(0, len(reading[0])-sequence_length-1)\n",
    "        x.append(reading[:, i:i+sequence_length])\n",
    "        y.append(0)\n",
    "        \n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    # change the shape of x from (normal_sample_size, dim, sequence_length) to (normal_sample_size, sequence_length, dim)\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    \n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "def get_mixed_data(dim, sequence_length, mixed_sample_size, \n",
    "                   noise_scale, abnormal_ratio, anomaly_magnitude, **kwargs):\n",
    "    '''\n",
    "    x contains the sequences draw from the normal/abnormal readings. \n",
    "    x has the shape of (mixed_sample_size, sequence_length, dim)\n",
    "    \n",
    "    y contains the label of if the sequence is an anomaly. y=1 means the sequence is abnormal.\n",
    "    '''\n",
    "    window_size = 13 # anomaly windows size\n",
    "    window_std = 2 # anomaly standard deviation\n",
    "    anomaly_dim = 0\n",
    "    \n",
    "    x, y = get_normal_data(dim, sequence_length, mixed_sample_size, noise_scale)\n",
    "    for i in range(mixed_sample_size):\n",
    "        isAnomaly = np.random.choice([0, 1], p=[1-abnormal_ratio, abnormal_ratio])\n",
    "        if isAnomaly == 1:\n",
    "            anomaly = anomaly_magnitude * signal.gaussian(window_size, std=window_std)\n",
    "            idx = np.random.randint(0, sequence_length-window_size)\n",
    "            x[i, idx:idx+window_size, anomaly_dim] += anomaly\n",
    "            y[i] = 1\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim':1, 'sequence_length':37, 'noise_scale':0.05, \n",
    "          'normal_sample_size':8192, 'mixed_sample_size':2048, 'abnormal_ratio':0.5, 'anomaly_magnitude':0.5}\n",
    "\n",
    "np.random.seed(9527)\n",
    "x_train, y_train = get_normal_data(**params)\n",
    "x_test, y_test = get_mixed_data(**params)\n",
    "\n",
    "zero_train = np.zeros((x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "zero_test = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_tf_model_fc(dim, sequence_length):\n",
    "    input_sequence = tf.keras.layers.Input(shape=(dim*sequence_length, ), name='input_sequence')\n",
    "    x = tf.keras.layers.Dense(units=128, activation='relu')(input_sequence)\n",
    "    decoded = tf.keras.layers.Dense(units=dim*sequence_length)(x)\n",
    "\n",
    "    diff = tf.keras.layers.Subtract(name='encoding_error')([input_sequence, decoded])\n",
    "    model = tf.keras.Model(input_sequence, diff)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8192 samples, validate on 1039 samples\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "8192/8192 [==============================] - 1s 157us/sample - loss: 0.0611 - val_loss: 0.0035\n",
      "Epoch 2/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 3/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 4/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 5/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0027 - val_loss: 0.0028\n",
      "Epoch 6/20\n",
      "8192/8192 [==============================] - 0s 38us/sample - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 7/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0024 - val_loss: 0.0026\n",
      "Epoch 8/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 9/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0021 - val_loss: 0.0023\n",
      "Epoch 10/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 11/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 12/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 13/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 14/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 15/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 16/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 17/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 18/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 19/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 20/20\n",
      "8192/8192 [==============================] - 0s 33us/sample - loss: 9.8970e-04 - val_loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "x_train_flatten = np.reshape(x_train, (x_train.shape[0], np.prod(x_train.shape[1:])))\n",
    "x_test_flatten = np.reshape(x_test, (x_test.shape[0], np.prod(x_test.shape[1:])))\n",
    "\n",
    "zero_train_flatten = np.reshape(zero_train, (zero_train.shape[0], np.prod(zero_train.shape[1:])))\n",
    "zero_test_flatten = np.reshape(zero_test, (zero_test.shape[0], np.prod(zero_test.shape[1:])))\n",
    "\n",
    "model = get_tf_model_fc(dim=params['dim'], sequence_length=params['sequence_length'])\n",
    "\n",
    "model.fit(x_train_flatten, zero_train_flatten, batch_size=64, epochs=20, \n",
    "          validation_data=(x_test_flatten[y_test==0], zero_test_flatten[y_test==0]))\n",
    "\n",
    "pred_origin_model = model.predict(x_test_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the model to a .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference:\n",
    "# https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.compat.v1.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-5a3781e77539>:28: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 21 variables.\n",
      "INFO:tensorflow:Converted 21 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Anomaly_detection_model_v2.pb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.compat.v1.keras.backend.get_session()\n",
    "frozen_graph = freeze_session(sess, output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "output_tf_model = 'Anomaly_detection_model_v2.pb'\n",
    "# Finally we serialize and dump the output graph to the filesystem\n",
    "tf.io.write_graph(frozen_graph, \"./\", output_tf_model, as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and use it in TensorFlow\n",
    "Reference: https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.gfile.GFile('./' + output_tf_model, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence => Placeholder\n"
     ]
    }
   ],
   "source": [
    "nodes = [n.name + ' => ' +  n.op for n in graph_def.node if n.op in ('Placeholder')]\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Define input tensor\n",
    "    input_tensor = tf.placeholder(np.float32, shape = [None, params['sequence_length']*params['dim']])\n",
    "    tf.import_graph_def(graph_def, {'input_sequence': input_tensor})\n",
    "graph.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder\n",
      "import/input_sequence\n",
      "import/dense/kernel\n",
      "import/dense/bias\n",
      "import/dense/MatMul/ReadVariableOp\n",
      "import/dense/MatMul\n",
      "import/dense/BiasAdd/ReadVariableOp\n",
      "import/dense/BiasAdd\n",
      "import/dense/Relu\n",
      "import/dense_1/kernel\n",
      "import/dense_1/bias\n",
      "import/dense_1/MatMul/ReadVariableOp\n",
      "import/dense_1/MatMul\n",
      "import/dense_1/BiasAdd/ReadVariableOp\n",
      "import/dense_1/BiasAdd\n",
      "import/encoding_error/sub\n",
      "import/Adam/iterations\n",
      "import/Adam/lr\n",
      "import/Adam/beta_1\n",
      "import/Adam/beta_2\n",
      "import/Adam/decay\n",
      "import/training/Adam/Variable\n",
      "import/training/Adam/Variable_1\n",
      "import/training/Adam/Variable_2\n",
      "import/training/Adam/Variable_3\n",
      "import/training/Adam/Variable_4\n",
      "import/training/Adam/Variable_5\n",
      "import/training/Adam/Variable_6\n",
      "import/training/Adam/Variable_7\n",
      "import/training/Adam/Variable_8\n",
      "import/training/Adam/Variable_9\n",
      "import/training/Adam/Variable_10\n",
      "import/training/Adam/Variable_11\n"
     ]
    }
   ],
   "source": [
    "layers = [op.name for op in graph.get_operations()]\n",
    "for layer in layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "output_tensor = graph.get_tensor_by_name(\"import/encoding_error/sub:0\")\n",
    "pred_loaded_model = sess.run(output_tensor, feed_dict={input_tensor: x_test_flatten})\n",
    "\n",
    "# The difference should be very close to 0\n",
    "np.sum(np.abs(pred_loaded_model - pred_origin_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
