{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(9527)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_reading(noise_scale=0.1):\n",
    "    t = np.linspace(0, 10.0, 1001)\n",
    "    signal = np.sin(2*np.pi*t) + np.sin(4*np.pi*t)\n",
    "    \n",
    "    noise = np.random.normal(0, noise_scale, len(t))\n",
    "    reading = signal + noise\n",
    "    \n",
    "    return reading\n",
    "\n",
    "def get_abnormal_reading(noise_scale=0.1):\n",
    "    t = np.linspace(0, 10.0, 1001)\n",
    "    signal = np.sin(2*np.pi*t) + np.sin(4*np.pi*t) + np.sin(10*np.pi*t)\n",
    "    \n",
    "    noise = np.random.normal(0, noise_scale, len(t))\n",
    "    reading = signal + noise\n",
    "    \n",
    "    return reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_data(sample_size=1024, sequence_length=100, noise_scale=0.1):\n",
    "    '''\n",
    "    x contains the sequences draw from the normal readings\n",
    "    y contains the label of if the sequence is an anomaly. In this case, all labels in y are 0.\n",
    "    '''\n",
    "    reading = get_normal_reading(noise_scale=noise_scale)\n",
    "      \n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(sample_size):\n",
    "        i = np.random.randint(0, len(reading)-sequence_length-1)\n",
    "        x.append(reading[i:i+sequence_length])\n",
    "        y.append(0)\n",
    "        \n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixed_data(sample_size=1024, sequence_length=100, noise_scale=0.1, abnormal_ratio=0.01):\n",
    "    '''\n",
    "    x contains the sequences draw from the normal/abnormal readings.\n",
    "    y contains the label of if the sequence is an anomaly. y=1 means the sequence is abnormal.\n",
    "    '''\n",
    "    normal_reading = get_normal_reading(noise_scale=noise_scale)\n",
    "    abnormal_reading = get_abnormal_reading(noise_scale=noise_scale)\n",
    "        \n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(sample_size):\n",
    "        isAnomaly = np.random.choice([0, 1], p=[1-abnormal_ratio, abnormal_ratio])\n",
    "        if isAnomaly == 0:\n",
    "            i = np.random.randint(0, len(normal_reading)-sequence_length-1)\n",
    "            x.append(normal_reading[i:i+sequence_length])\n",
    "        else:\n",
    "            i = np.random.randint(0, len(abnormal_reading)-sequence_length-1)\n",
    "            x.append(abnormal_reading[i:i+sequence_length])\n",
    "        y.append(isAnomaly)\n",
    "        \n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# Input a sequence and output the sequence itself\n",
    "def get_keras_model(sequence_length):\n",
    "    Input_sequence = Input(shape=(sequence_length, ))\n",
    "    x = Dense(64, activation='relu')(Input_sequence)\n",
    "    decoded = Dense(sequence_length)(x)\n",
    "    \n",
    "    autoencoder = Model(Input_sequence, output=decoded)\n",
    "    autoencoder.compile(optimizer='RMSProp', loss='mse')\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Input a sequence and output the sequence itself\n",
    "def get_tf_model(sequence_length):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    Input_sequence = tf.keras.layers.Input(shape=(sequence_length, ), name='input_sequence')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(Input_sequence)\n",
    "    decoded = tf.keras.layers.Dense(sequence_length, name='output_sequence')(x)\n",
    "    \n",
    "    autoencoder = tf.keras.Model(Input_sequence, decoded)\n",
    "    autoencoder.compile(optimizer='RMSProp', loss='mse')\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on normal data and test on mixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2048 samples, validate on 503 samples\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "2048/2048 [==============================] - 1s 530us/sample - loss: 0.6080 - val_loss: 0.1774\n",
      "Epoch 2/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0875 - val_loss: 0.0420\n",
      "Epoch 3/20\n",
      "2048/2048 [==============================] - 0s 33us/sample - loss: 0.0286 - val_loss: 0.0230\n",
      "Epoch 4/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0183 - val_loss: 0.0180\n",
      "Epoch 5/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0154 - val_loss: 0.0164\n",
      "Epoch 6/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0141 - val_loss: 0.0155\n",
      "Epoch 7/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0132 - val_loss: 0.0145\n",
      "Epoch 8/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0126 - val_loss: 0.0140\n",
      "Epoch 9/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0121 - val_loss: 0.0135\n",
      "Epoch 10/20\n",
      "2048/2048 [==============================] - 0s 33us/sample - loss: 0.0117 - val_loss: 0.0129\n",
      "Epoch 11/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0113 - val_loss: 0.0133\n",
      "Epoch 12/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0109 - val_loss: 0.0126\n",
      "Epoch 13/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0107 - val_loss: 0.0120\n",
      "Epoch 14/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 15/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0101 - val_loss: 0.0115\n",
      "Epoch 16/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 17/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0095 - val_loss: 0.0108\n",
      "Epoch 18/20\n",
      "2048/2048 [==============================] - 0s 33us/sample - loss: 0.0093 - val_loss: 0.0107\n",
      "Epoch 19/20\n",
      "2048/2048 [==============================] - 0s 32us/sample - loss: 0.0091 - val_loss: 0.0108\n",
      "Epoch 20/20\n",
      "2048/2048 [==============================] - 0s 31us/sample - loss: 0.0088 - val_loss: 0.0103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad75c1db10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 37\n",
    "\n",
    "x_train, y_train = get_normal_data(sample_size=2048, sequence_length=sequence_length)\n",
    "x_test, y_test = get_mixed_data(sequence_length=sequence_length, abnormal_ratio=0.5)\n",
    "\n",
    "model = get_tf_model(sequence_length=sequence_length)\n",
    "model.fit(x_train, x_train, batch_size=64, epochs=20, validation_data=(x_test[y_test==0], x_test[y_test==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6623678 4.5323687\n",
      "12.582756 25.924002\n"
     ]
    }
   ],
   "source": [
    "x_pred = model.predict(x_test)\n",
    "\n",
    "error = [np.sum(np.abs(pred - test)) for (pred, test) in zip(x_pred, x_test)]\n",
    "error = np.array(error)\n",
    "\n",
    "error_normal = error[y_test == 0]\n",
    "error_abnormal = error[y_test == 1]\n",
    "\n",
    "print(min(error_normal), max(error_normal))\n",
    "print(min(error_abnormal), max(error_abnormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the model to a .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference:\n",
    "# https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.compat.v1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.compat.v1.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-5a3781e77539>:28: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /data/anaconda/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 12 variables.\n",
      "INFO:tensorflow:Converted 12 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./AnomalyDetection_BaseModel.pb'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.compat.v1.keras.backend.get_session()\n",
    "frozen_graph = freeze_session(sess, output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "output_tf_model = 'AnomalyDetection_BaseModel.pb'\n",
    "# Finally we serialize and dump the output graph to the filesystem\n",
    "tf.io.write_graph(frozen_graph, \"./\", output_tf_model, as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and use it in TensorFlow\n",
    "Reference: https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with tf.gfile.GFile('./' + output_tf_model, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sequence => Placeholder\n",
      "dense/kernel => Const\n",
      "dense/bias => Const\n",
      "dense/MatMul/ReadVariableOp => Identity\n",
      "dense/MatMul => MatMul\n",
      "dense/BiasAdd/ReadVariableOp => Identity\n",
      "dense/BiasAdd => BiasAdd\n",
      "dense/Relu => Relu\n",
      "output_sequence/kernel => Const\n",
      "output_sequence/bias => Const\n",
      "output_sequence/MatMul/ReadVariableOp => Identity\n",
      "output_sequence/MatMul => MatMul\n",
      "output_sequence/BiasAdd/ReadVariableOp => Identity\n",
      "output_sequence/BiasAdd => BiasAdd\n",
      "RMSprop/lr => Const\n",
      "RMSprop/rho => Const\n",
      "RMSprop/decay => Const\n",
      "RMSprop/iterations => Const\n",
      "training/RMSprop/Variable => Const\n",
      "training/RMSprop/Variable_1 => Const\n",
      "training/RMSprop/Variable_2 => Const\n",
      "training/RMSprop/Variable_3 => Const\n"
     ]
    }
   ],
   "source": [
    "nodes = [n.name + ' => ' +  n.op for n in graph_def.node]\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Define input tensor\n",
    "    input_tensor = tf.placeholder(np.float32, shape = [None, sequence_length])\n",
    "    tf.import_graph_def(graph_def, {'input_sequence': input_tensor})\n",
    "graph.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder\n",
      "import/input_sequence\n",
      "import/dense/kernel\n",
      "import/dense/bias\n",
      "import/dense/MatMul/ReadVariableOp\n",
      "import/dense/MatMul\n",
      "import/dense/BiasAdd/ReadVariableOp\n",
      "import/dense/BiasAdd\n",
      "import/dense/Relu\n",
      "import/output_sequence/kernel\n",
      "import/output_sequence/bias\n",
      "import/output_sequence/MatMul/ReadVariableOp\n",
      "import/output_sequence/MatMul\n",
      "import/output_sequence/BiasAdd/ReadVariableOp\n",
      "import/output_sequence/BiasAdd\n",
      "import/RMSprop/lr\n",
      "import/RMSprop/rho\n",
      "import/RMSprop/decay\n",
      "import/RMSprop/iterations\n",
      "import/training/RMSprop/Variable\n",
      "import/training/RMSprop/Variable_1\n",
      "import/training/RMSprop/Variable_2\n",
      "import/training/RMSprop/Variable_3\n"
     ]
    }
   ],
   "source": [
    "layers = [op.name for op in graph.get_operations()]\n",
    "for layer in layers:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_temp, y_temp = get_mixed_data(sample_size = 10, sequence_length=sequence_length, abnormal_ratio=0.5)\n",
    "\n",
    "sess = tf.Session(graph=graph)\n",
    "output_tensor = graph.get_tensor_by_name(\"import/output_sequence/BiasAdd:0\")\n",
    "pred_loaded_model = sess.run(output_tensor, feed_dict={input_tensor: x_temp})\n",
    "\n",
    "pred_origin_model = model.predict(x_temp)\n",
    "\n",
    "# The difference should be very close to 0\n",
    "np.sum(np.abs(pred_loaded_model - pred_origin_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
